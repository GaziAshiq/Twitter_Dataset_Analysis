{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-27T22:33:00.726607900Z",
     "start_time": "2023-05-27T22:32:59.669788800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gazia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gazia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download stopwords and punkt tokenizer for sentence splitting\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T22:33:26.006756400Z",
     "start_time": "2023-05-27T22:33:25.699949900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# load twitter_dataset\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv('twitter_dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T22:33:53.992111Z",
     "start_time": "2023-05-27T22:33:53.649724300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# combine all tweets text into a single list\n",
    "tweet_text = tweets['Text'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T22:35:50.842339Z",
     "start_time": "2023-05-27T22:35:50.833829200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# tokenize and preprocess the text\n",
    "tokens = [word_tokenize(text.lower()) for text in tweet_text]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [[word for word in token if word not in stop_words and word != '.'] for token in tokens]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T22:45:13.526066900Z",
     "start_time": "2023-05-27T22:45:09.686828600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(filtered_tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in filtered_tokens]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T22:45:15.709111600Z",
     "start_time": "2023-05-27T22:45:15.302302Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Apply LDA model\n",
    "num_topics = 10\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T22:45:20.876640Z",
     "start_time": "2023-05-27T22:45:16.541366400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.006*\"build\" + 0.004*\"reduce\" + 0.004*\"agency\" + 0.004*\"since\" + 0.003*\"central\" + 0.003*\"focus\" + 0.003*\"often\" + 0.003*\"middle\" + 0.003*\"order\" + 0.003*\"system\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.004*\"listen\" + 0.004*\"sit\" + 0.004*\"deal\" + 0.004*\"material\" + 0.004*\"write\" + 0.004*\"raise\" + 0.003*\"sound\" + 0.003*\"someone\" + 0.003*\"trip\" + 0.003*\"major\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.005*\"career\" + 0.005*\"organization\" + 0.005*\"hot\" + 0.004*\"general\" + 0.004*\"test\" + 0.004*\"various\" + 0.004*\"painting\" + 0.004*\"well\" + 0.004*\"base\" + 0.004*\"manage\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.005*\"bar\" + 0.004*\"support\" + 0.004*\"city\" + 0.004*\"way\" + 0.004*\"hard\" + 0.003*\"sport\" + 0.003*\"admit\" + 0.003*\"american\" + 0.003*\"quickly\" + 0.003*\"final\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.004*\"item\" + 0.004*\"cup\" + 0.003*\"grow\" + 0.003*\"soon\" + 0.003*\"senior\" + 0.003*\"measure\" + 0.003*\"buy\" + 0.003*\"somebody\" + 0.003*\"father\" + 0.003*\"water\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.005*\"decide\" + 0.005*\"protect\" + 0.005*\"technology\" + 0.004*\"true\" + 0.004*\"simply\" + 0.004*\"arm\" + 0.004*\"shoulder\" + 0.004*\"meet\" + 0.004*\"able\" + 0.004*\"instead\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.005*\"administration\" + 0.004*\"shake\" + 0.004*\"tough\" + 0.004*\"idea\" + 0.004*\"break\" + 0.004*\"marriage\" + 0.004*\"ready\" + 0.004*\"sport\" + 0.003*\"avoid\" + 0.003*\"source\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.004*\"wrong\" + 0.004*\"say\" + 0.004*\"toward\" + 0.004*\"long\" + 0.003*\"born\" + 0.003*\"one\" + 0.003*\"respond\" + 0.003*\"discover\" + 0.003*\"science\" + 0.003*\"specific\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.004*\"decision\" + 0.004*\"political\" + 0.004*\"add\" + 0.004*\"attack\" + 0.004*\"congress\" + 0.003*\"believe\" + 0.003*\"industry\" + 0.003*\"play\" + 0.003*\"outside\" + 0.003*\"miss\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.006*\"common\" + 0.004*\"arrive\" + 0.004*\"option\" + 0.004*\"wonder\" + 0.004*\"lead\" + 0.004*\"especially\" + 0.004*\"opportunity\" + 0.003*\"trial\" + 0.003*\"natural\" + 0.003*\"past\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print topics\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f'Topic: {idx} \\nWords: {topic}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T22:45:21.911700700Z",
     "start_time": "2023-05-27T22:45:21.893286700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
